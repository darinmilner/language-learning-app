pipeline {
    agent any

    def tfLib = evaluate readTrusted("groovy/terraformUtils.groovy")
    def commonLib = evaluate readTrusted("groovy/common.groovy")
    def lambdas = load 'groovy/lambdas.groovy'

    parameters {
        string(name: 'REPO_URL', defaultValue: 'https://github.com/your-username/your-repo.git', description: 'GitHub Repository URL')
        string(name: 'BUCKET_NAME', defaultValue: 'jenkins-terraform-bucket-123456', description: 'S3 Bucket Name')
        choice(name: 'AWS_REGION', choices: ['us-east-1', 'us-west-1', 'eu-west-1'], description: 'AWS Region')
        booleanParam(name: 'DESTROY_INFRASTRUCTURE', defaultValue: false, description: 'Destroy infrastructure instead of provisioning it')
        booleanParam(
            name: 'DEPLOY_LAMBDAS',
            defaultValue: false,
            description: 'Check to deploy all Lambda modules, uncheck to deploy core infrastructure'
        )
    }

    environment {
        REGION = "${params.AWS_REGION}"
        BUCKET = "${params.BUCKET_NAME}"
        TF_STATE_KEY = "terraform.tfstate"
        HASH_KEY = "lambda_hash.txt"  // Key for the hash file in S3
        TF_DIR = 'Deployment/Terraform'
        NOTIFY_EMAIL = "youremail"
    }

    stages {
        stage("Clone Repo") {
            steps {
                echo "Cloning ${params.REPO_URL}"
                git url: "${params.REPO_URL}"
            }
        }

         stage("Print Selected AWS Region") {
            steps {
                echo "User selected AWS region: ${REGION}"
            }
        }

       stage('Setup AWS CLI') {
            steps {
                sh '''
                    python3 scripts/aws_setup.py
                    aws --version
                '''
            }
        }

        stage('Verify Login') {
            steps {
                sh 'aws sts get-caller-identity'
            }
        }

        stage('Generate Hash') {
            steps {
                checkout scm
                sh "chmod +x generate_hash.sh && ./scripts/generate_hash.sh"
            }
        }
    
        stage('Download Previous Hash') {
            steps {
                script {
                // Download previous hash from S3 (if exists)
                sh """
                    aws s3 cp "s3://${env.S3_BUCKET}/${env.HASH_KEY}" previous_hash.txt || true
                """
                }
            }
        }
    
        stage('Compare Hashes') {
            steps {
                script {
                    currentHash = readFile('lambda_hash.txt').trim()
                    previousHash = fileExists('previous_hash.txt') ? readFile('previous_hash.txt').trim() : ""
                    
                    if (currentHash == previousHash) {
                        // Skip deployment if hashes match
                        echo "Lambda code unchanged. Skipping Terraform deployment."
                        currentBuild.result = 'SUCCESS'
                        error("STOP_PIPELINE") // Halt pipeline gracefully
                    } else {
                        echo "Lambda code changed. Proceeding with deployment."
                    }
                }
            }
        }

        // runs python script
        stage('Detect Changes') {
            if (params.DEPLOY_LAMBDAS){
                steps {
                    script {
                        // Define your Lambda directories
                        lambdaModules = ["lambda1" "lambda2" "lambda3"]
                        
                        // Run change detection
                        def changeStatus = sh(
                            script: "python3 hash_check.py --s3-bucket ${env.S3_BUCKET} --key-prefix ${env.KEY_PREFIX} ${LAMBDA_DIRS}",
                            returnStatus: true
                        )

                        buildLambdaModules(lambdaModules)
                        
                        // Store change status in environment variable
                        env.CHANGES_DETECTED = (changeStatus == 1)
                    }
                }
            }        
        }

         stage('Validate Input') {
            steps {
                script {
                    lambdas.validate(params.LAMBDA_FOLDER)
                }
            }
        }
        
        stage('Setup Environment') {
            steps {
                script {
                    lambdas.setupEnv(params.LAMBDA_FOLDER, env.VENV_DIR)
                }
            }
        }
        
        stage('Install Dependencies') {
            steps {
                script {
                    lambdas.installDeps(params.LAMBDA_FOLDER, env.VENV_DIR)
                }
            }
        }
        
        stage('Package Lambda') {
            steps {
                script {
                    lambdas.package(params.LAMBDA_FOLDER, env.VENV_DIR)
                }
            }
        }
        
        stage('Upload to S3') {
            steps {
                script {
                    lambdas.uploadS3(
                        params.LAMBDA_FOLDER, 
                        env.S3_BUCKET, 
                        env.AWS_REGION, 
                        env.AWS_CREDENTIALS_ID
                    )
                }
            }
        }
    
        stage("Install Terraform") {
            steps {
                script {
                    tfLib.downloadTerraform()
                }
            }
        }

        stage('Production Approval') {
            when { 
                expression { params.DEPLOY_LAMBDAS && ENV == 'prod' }
            }
            steps {
                input message: 'Approve production deployment?', ok: 'Deploy'
            }
        }

        // Apply Terraform 
        stage('Terraform Apply') {
            if (!params.DEPLOY_LAMBDAS){
                steps {
                    script {
                        sh """
                            terraform init -backend-config="bucket=${env.S3_BUCKET}"
                            terraform apply -auto-approve
                        """
                    }
                }
            }
        }

        stage('Upload New Hash') {
            steps {
                script {
                // Upload the new hash to S3
                sh "aws s3 cp lambda_hash.txt s3://${env.S3_BUCKET}/${env.HASH_KEY}"
                }
            }
        }

        // TODO: Add Email validation (AWS SNS)
        // stage('Initialize & Validate Terraform') {
        //     Result: SUCCESS
        //     URL: ${env.BUILD_URL}
        //     """,
        //         to: "${env.NOTIFY_EMAIL}"
        //     """
        // }

        // failure {
        //     emailext subject: "âŒ Jenkins Build FAILED: ${env.JOB_NAME} #${env.BUILD_NUMBER}",
        //              body: """Build failed.

        //     Job: ${env.JOB_NAME}
        //     Build: #${env.BUILD_NUMBER}
        //     Result: FAILURE
        //     URL: ${env.BUILD_URL}
        //     """,
        //         to: "${env.NOTIFY_EMAIL}"
        // }
    }
}

// Deploy all Lambda modules
def buildLambdaModules(modules) {
    stage('Deploy Lambda Modules') {
        steps {
            script {
                modules.each { module ->
                    stage("Deploy: ${module}") {
                        dir(module) {
                             script {
                                sh """
                                    terraform init -backend-config="bucket=${env.S3_BUCKET}"
                                    terraform apply -auto-approve
                                """
                            }
                        }
                    }
                }
            }
        }
    }
}




